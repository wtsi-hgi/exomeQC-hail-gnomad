{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip-installed Hail requires additional configuration options in Spark referring\n",
      "  to the path to the Hail Python module directory HAIL_DIR,\n",
      "  e.g. /path/to/python/site-packages/hail:\n",
      "    spark.jars=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.7\n",
      "SparkUI available at http://spark-master:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.60-de1845e1c2f6\n",
      "LOGGING: writing to /home/ubuntu/data/tmp/scripts/sanger_gnomad_hail_qc/notebooks/hail-20210119-1531-0.2.60-de1845e1c2f6.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n",
       "  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import hail as hl\n",
    "import pyspark\n",
    "import bokeh\n",
    "import logging\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pickle \n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, Counter, List, Optional, Tuple, Union,Dict,Set\n",
    "from hail.plot import show, output_notebook\n",
    "from bokeh.palettes import d3  # pylint: disable=no-name-in-module\n",
    "from bokeh.models import Plot, Row, Span, NumeralTickFormatter, LabelSet\n",
    "from gnomad.utils.plotting import *\n",
    "from typing import Set, Tuple\n",
    "\n",
    "tmp_dir = \"hdfs://spark-master:9820/\"\n",
    "temp_dir = \"file:///home/ubuntu/data/tmp\"\n",
    "plot_dir = \"/home/ubuntu/data/tmp\"\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "hadoop_config = sc._jsc.hadoopConfiguration()\n",
    "hadoop_config.set(\"fs.s3a.access.key\", \"8YY584J59H7Q6AVKHSU8\")\n",
    "hadoop_config.set(\"fs.s3a.secret.key\", \"P8vePa7JUvxKXX2me9ti1cGujgYWMoimAwx4mMlM\")\n",
    "hadoop_config.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_config.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hl.init(sc=sc, tmp_dir=tmp_dir, default_reference='GRCh38')\n",
    "output_notebook()\n",
    "logging.basicConfig(format=\"%(levelname)s (%(name)s %(lineno)s): %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FatalError",
     "evalue": "SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/\n\nJava stack trace:\njava.nio.file.AccessDeniedException: DDD-ELGH-UKBB-exomes: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat is.hail.io.fs.HadoopFS.getFileSystem(HadoopFS.scala:98)\n\tat is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:139)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:121)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:120)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat is.hail.io.fs.HadoopFS.globAll(HadoopFS.scala:126)\n\tat is.hail.io.vcf.MatrixVCFReader$.apply(LoadVCF.scala:1557)\n\tat is.hail.io.vcf.MatrixVCFReader$.fromJValue(LoadVCF.scala:1652)\n\tat is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:91)\n\tat is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1693)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.utils.StackSafe$More.advance(StackSafe.scala:64)\n\tat is.hail.utils.StackSafe$.run(StackSafe.scala:16)\n\tat is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$.parse(Parser.scala:1946)\n\tat is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1959)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:604)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:603)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23)\n\tat is.hail.utils.package$.using(package.scala:618)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:247)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:603)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:602)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59)\n\tat is.hail.backend.spark.SparkBackend.parse_matrix_ir(SparkBackend.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\norg.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1166)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:762)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:724)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4368)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4315)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1344)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1284)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat is.hail.io.fs.HadoopFS.getFileSystem(HadoopFS.scala:98)\n\tat is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:139)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:121)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:120)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat is.hail.io.fs.HadoopFS.globAll(HadoopFS.scala:126)\n\tat is.hail.io.vcf.MatrixVCFReader$.apply(LoadVCF.scala:1557)\n\tat is.hail.io.vcf.MatrixVCFReader$.fromJValue(LoadVCF.scala:1652)\n\tat is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:91)\n\tat is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1693)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.utils.StackSafe$More.advance(StackSafe.scala:64)\n\tat is.hail.utils.StackSafe$.run(StackSafe.scala:16)\n\tat is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$.parse(Parser.scala:1946)\n\tat is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1959)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:604)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:603)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23)\n\tat is.hail.utils.package$.using(package.scala:618)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:247)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:603)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:602)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59)\n\tat is.hail.backend.spark.SparkBackend.parse_matrix_ir(SparkBackend.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\ncom.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/\n\tat com.amazonaws.internal.EC2CredentialsUtils.readResource(EC2CredentialsUtils.java:125)\n\tat com.amazonaws.internal.EC2CredentialsUtils.readResource(EC2CredentialsUtils.java:87)\n\tat com.amazonaws.auth.InstanceProfileCredentialsProvider$InstanceMetadataCredentialsEndpointProvider.getCredentialsEndpoint(InstanceProfileCredentialsProvider.java:189)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.fetchCredentials(EC2CredentialsFetcher.java:122)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.getCredentials(EC2CredentialsFetcher.java:82)\n\tat com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:164)\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1166)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:762)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:724)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4368)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4315)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1344)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1284)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat is.hail.io.fs.HadoopFS.getFileSystem(HadoopFS.scala:98)\n\tat is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:139)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:121)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:120)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat is.hail.io.fs.HadoopFS.globAll(HadoopFS.scala:126)\n\tat is.hail.io.vcf.MatrixVCFReader$.apply(LoadVCF.scala:1557)\n\tat is.hail.io.vcf.MatrixVCFReader$.fromJValue(LoadVCF.scala:1652)\n\tat is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:91)\n\tat is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1693)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.utils.StackSafe$More.advance(StackSafe.scala:64)\n\tat is.hail.utils.StackSafe$.run(StackSafe.scala:16)\n\tat is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$.parse(Parser.scala:1946)\n\tat is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1959)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:604)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:603)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23)\n\tat is.hail.utils.package$.using(package.scala:618)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:247)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:603)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:602)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59)\n\tat is.hail.backend.spark.SparkBackend.parse_matrix_ir(SparkBackend.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\n\nHail version: 0.2.60-de1845e1c2f6\nError summary: SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-acfe47649898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_vcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://DDD-ELGH-UKBB-exomes/ancestry/WES_AKT_1kg_intersection.vcf.bgz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference_genome\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GRCh38'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-1433>\u001b[0m in \u001b[0;36mimport_vcf\u001b[0;34m(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci, entry_float_type, filter, find_replace, n_partitions, block_size, _partitions)\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/methods/impex.py\u001b[0m in \u001b[0;36mimport_vcf\u001b[0;34m(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci, entry_float_type, filter, find_replace, n_partitions, block_size, _partitions)\u001b[0m\n\u001b[1;32m   2246\u001b[0m                                 \u001b[0mskip_invalid_loci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_bgz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_replace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                                 _partitions)\n\u001b[0;32m-> 2248\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMatrixTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMatrixRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/matrixtable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mir)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entry_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_row_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_col_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/ir/base_ir.py\u001b[0m in \u001b[0;36mtyp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/ir/matrix_ir.py\u001b[0m in \u001b[0;36m_compute_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mmatrix_type\u001b[0;34m(self, mir)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatrix_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mjir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_matrix_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36m_to_java_matrix_ir\u001b[0;34m(self, ir)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_to_java_matrix_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_matrix_ir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_to_java_blockmatrix_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36m_to_java_ir\u001b[0;34m(self, ir, parse)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSERenderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_at_jir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;31m# FIXME parse should be static\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36m_parse_matrix_ir\u001b[0;34m(self, code, ref_map, ir_map)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_matrix_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_matrix_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_blockmatrix_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/backend/py4j_backend.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n\u001b[1;32m     31\u001b[0m                                  \u001b[0;34m'Hail version: %s\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                  'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n",
      "\u001b[0;31mFatalError\u001b[0m: SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/\n\nJava stack trace:\njava.nio.file.AccessDeniedException: DDD-ELGH-UKBB-exomes: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat is.hail.io.fs.HadoopFS.getFileSystem(HadoopFS.scala:98)\n\tat is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:139)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:121)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:120)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat is.hail.io.fs.HadoopFS.globAll(HadoopFS.scala:126)\n\tat is.hail.io.vcf.MatrixVCFReader$.apply(LoadVCF.scala:1557)\n\tat is.hail.io.vcf.MatrixVCFReader$.fromJValue(LoadVCF.scala:1652)\n\tat is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:91)\n\tat is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1693)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.utils.StackSafe$More.advance(StackSafe.scala:64)\n\tat is.hail.utils.StackSafe$.run(StackSafe.scala:16)\n\tat is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$.parse(Parser.scala:1946)\n\tat is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1959)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:604)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:603)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23)\n\tat is.hail.utils.package$.using(package.scala:618)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:247)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:603)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:602)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59)\n\tat is.hail.backend.spark.SparkBackend.parse_matrix_ir(SparkBackend.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\norg.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1166)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:762)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:724)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4368)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4315)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1344)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1284)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat is.hail.io.fs.HadoopFS.getFileSystem(HadoopFS.scala:98)\n\tat is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:139)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:121)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:120)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat is.hail.io.fs.HadoopFS.globAll(HadoopFS.scala:126)\n\tat is.hail.io.vcf.MatrixVCFReader$.apply(LoadVCF.scala:1557)\n\tat is.hail.io.vcf.MatrixVCFReader$.fromJValue(LoadVCF.scala:1652)\n\tat is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:91)\n\tat is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1693)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.utils.StackSafe$More.advance(StackSafe.scala:64)\n\tat is.hail.utils.StackSafe$.run(StackSafe.scala:16)\n\tat is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$.parse(Parser.scala:1946)\n\tat is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1959)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:604)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:603)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23)\n\tat is.hail.utils.package$.using(package.scala:618)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:247)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:603)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:602)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59)\n\tat is.hail.backend.spark.SparkBackend.parse_matrix_ir(SparkBackend.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\ncom.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/\n\tat com.amazonaws.internal.EC2CredentialsUtils.readResource(EC2CredentialsUtils.java:125)\n\tat com.amazonaws.internal.EC2CredentialsUtils.readResource(EC2CredentialsUtils.java:87)\n\tat com.amazonaws.auth.InstanceProfileCredentialsProvider$InstanceMetadataCredentialsEndpointProvider.getCredentialsEndpoint(InstanceProfileCredentialsProvider.java:189)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.fetchCredentials(EC2CredentialsFetcher.java:122)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.getCredentials(EC2CredentialsFetcher.java:82)\n\tat com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:164)\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1166)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:762)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:724)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4368)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4315)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1344)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1284)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat is.hail.io.fs.HadoopFS.getFileSystem(HadoopFS.scala:98)\n\tat is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:139)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:121)\n\tat is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:120)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat is.hail.io.fs.HadoopFS.globAll(HadoopFS.scala:126)\n\tat is.hail.io.vcf.MatrixVCFReader$.apply(LoadVCF.scala:1557)\n\tat is.hail.io.vcf.MatrixVCFReader$.fromJValue(LoadVCF.scala:1652)\n\tat is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:91)\n\tat is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1693)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.expr.ir.IRParser$$anonfun$matrix_ir$1.apply(Parser.scala:1619)\n\tat is.hail.utils.StackSafe$More.advance(StackSafe.scala:64)\n\tat is.hail.utils.StackSafe$.run(StackSafe.scala:16)\n\tat is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$1.apply(Parser.scala:1959)\n\tat is.hail.expr.ir.IRParser$.parse(Parser.scala:1946)\n\tat is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1959)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:604)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1$$anonfun$apply$22.apply(SparkBackend.scala:603)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23)\n\tat is.hail.utils.package$.using(package.scala:618)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:247)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:603)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$parse_matrix_ir$1.apply(SparkBackend.scala:602)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59)\n\tat is.hail.backend.spark.SparkBackend.parse_matrix_ir(SparkBackend.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\n\nHail version: 0.2.60-de1845e1c2f6\nError summary: SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/"
     ]
    }
   ],
   "source": [
    "mt = hl.import_vcf(\"s3a://DDD-ELGH-UKBB-exomes/ancestry/WES_AKT_1kg_intersection.vcf.bgz\",reference_genome='GRCh38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
