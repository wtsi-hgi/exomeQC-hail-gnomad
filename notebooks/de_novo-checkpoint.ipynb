{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip-installed Hail requires additional configuration options in Spark referring\n",
      "  to the path to the Hail Python module directory HAIL_DIR,\n",
      "  e.g. /path/to/python/site-packages/hail:\n",
      "    spark.jars=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.5\n",
      "SparkUI available at http://spark-master:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.41-b8144dba46e6\n",
      "LOGGING: writing to /home/ubuntu/data/tmp/scripts/sanger_gnomad_hail_qc/notebooks/hail-20201123-1433-0.2.41-b8144dba46e6.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n",
       "  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import hail as hl\n",
    "import pyspark\n",
    "import bokeh\n",
    "import logging\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pickle \n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, Counter, List, Optional, Tuple, Union,Dict,Set\n",
    "from hail.plot import show, output_notebook\n",
    "from bokeh.palettes import d3  # pylint: disable=no-name-in-module\n",
    "from bokeh.models import Plot, Row, Span, NumeralTickFormatter, LabelSet\n",
    "from gnomad.utils.plotting import *\n",
    "from typing import Set, Tuple\n",
    "\n",
    "tmp_dir = \"hdfs://spark-master:9820/\"\n",
    "temp_dir = \"file:///home/ubuntu/data/tmp\"\n",
    "plot_dir = \"/home/ubuntu/data/tmp\"\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "hadoop_config = sc._jsc.hadoopConfiguration()\n",
    "hadoop_config.set(\"fs.s3a.access.key\", \"8YY584J59H7Q6AVKHSU8\")\n",
    "hadoop_config.set(\"fs.s3a.secret.key\", \"P8vePa7JUvxKXX2me9ti1cGujgYWMoimAwx4mMlM\")\n",
    "hadoop_config.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_config.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hl.init(sc=sc, tmp_dir=tmp_dir, default_reference='GRCh38')\n",
    "output_notebook()\n",
    "logging.basicConfig(format=\"%(levelname)s (%(name)s %(lineno)s): %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_gnomad=\"s3a://DDD-ELGH-UKBB-exomes/gnomad-AF/gnomad_3.0_sites_AF.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-23 12:04:31 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (user-specified)\n",
      "  Loading column 'f1' as type 'int32' (user-specified)\n",
      "  Loading column 'f2' as type 'str' (user-specified)\n",
      "  Loading column 'f3' as type 'str' (user-specified)\n",
      "  Loading column 'f4' as type 'str' (user-specified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ht_af=hl.import_table(af_gnomad, types={'f0':'str','f1':'int32', 'f2':'str','f3':'str','f4':'str'},no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_af=ht_af.annotate(chrom=ht_af.f0)\n",
    "ht_af=ht_af.annotate(position=ht_af.f1)\n",
    "ht_af=ht_af.annotate(ref=ht_af.f2)\n",
    "ht_af=ht_af.annotate(alt=ht_af.f3)\n",
    "ht_af=ht_af.annotate(smaf=ht_af.f4)\n",
    "ht_af=ht_af.select(ht_af.chrom, ht_af.position, ht_af.ref, ht_af.alt, ht_af.smaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_af = ht_af.key_by(\n",
    "    locus=hl.locus(ht_af.chrom, ht_af.position, reference_genome='GRCh38'), \n",
    "    alleles=[ht_af.ref, ht_af.alt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707950943"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht_af.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_af=ht_af.filter(ht_af.smaf !='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707818395"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht_af.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = ht_af.annotate(maf = hl.float64(ht_af.smaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'chrom': str \n",
      "    'position': int32 \n",
      "    'ref': str \n",
      "    'alt': str \n",
      "    'smaf': str \n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'maf': float64 \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht=ht.drop(ht.smaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-23 12:06:52 Hail: INFO: Coerced sorted dataset\n",
      "2020-11-23 12:07:50 Hail: INFO: wrote table with 707818395 rows in 655 partitions to hdfs://spark-master:9820//gnomad_v3-0_AF.ht\n"
     ]
    }
   ],
   "source": [
    "ht.write(f'{tmp_dir}/gnomad_v3-0_AF.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt=hl.read_matrix_table(f'{temp_dir}/ddd-elgh-ukbb/variant_qc/Sanger_cohorts_family_stats.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = f\"{temp_dir}/ddd-elgh-ukbb/variant_qc/DDD_trios.fam\"\n",
    "pedigree = hl.Pedigree.read(fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = hl.read_table(f'{temp_dir}/ddd-elgh-ukbb/variant_qc/gnomad_v3-0_AF.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt=mt.annotate_rows(gnomad_maf=priors[mt.row_key].maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FatalError",
     "evalue": "RemoteException: File /Sanger_cohorts_family_stats_gnomad_AF.mt/rows/rows/parts/part-06253-0-6253-3-061ac548-88a3-fba3-3556-d350a102eb9a could only be written to 0 of the 1 minReplication nodes. There are 25 datanode(s) running and 25 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\n\n\nJava stack trace:\njava.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:19)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:17)\n\tat is.hail.utils.package$.using(package.scala:600)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:17)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 6253 in stage 0.0 failed 4 times, most recent failure: Lost task 6253.3 in stage 0.0 (TID 6351, 192.168.252.152, executor 5): org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /Sanger_cohorts_family_stats_gnomad_AF.mt/rows/rows/parts/part-06253-0-6253-3-061ac548-88a3-fba3-3556-d350a102eb9a could only be written to 0 of the 1 minReplication nodes. There are 25 datanode(s) running and 25 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1491)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1388)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\n\tat com.sun.proxy.$Proxy21.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy22.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)\n\tat org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1866)\n\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:163)\n\tat is.hail.rvd.RVD.writeRowsSplit(RVD.scala:918)\n\tat is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:224)\n\tat is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:41)\n\tat is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:25)\n\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:653)\n\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:19)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:17)\n\tat is.hail.utils.package$.using(package.scala:600)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:17)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\norg.apache.hadoop.ipc.RemoteException: File /Sanger_cohorts_family_stats_gnomad_AF.mt/rows/rows/parts/part-06253-0-6253-3-061ac548-88a3-fba3-3556-d350a102eb9a could only be written to 0 of the 1 minReplication nodes. There are 25 datanode(s) running and 25 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1491)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1388)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\n\tat com.sun.proxy.$Proxy21.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy22.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)\n\tat org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1866)\n\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)\n\n\n\n\n\nHail version: 0.2.41-b8144dba46e6\nError summary: RemoteException: File /Sanger_cohorts_family_stats_gnomad_AF.mt/rows/rows/parts/part-06253-0-6253-3-061ac548-88a3-fba3-3556-d350a102eb9a could only be written to 0 of the 1 minReplication nodes. There are 25 datanode(s) running and 25 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3fe92a16bb01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{tmp_dir}/Sanger_cohorts_family_stats_gnomad_AF.mt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-1217>\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals, _drop_cols, _drop_rows)\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/matrixtable.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals, _drop_cols, _drop_rows)\u001b[0m\n\u001b[1;32m   2489\u001b[0m }\"\"\"\n\u001b[1;32m   2490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_read_if_exists\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhadoop_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{output}/_SUCCESS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2491\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage_locally\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstage_locally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_codec_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_codec_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2492\u001b[0m         return hl.read_matrix_table(output, _intervals=_intervals, _filter_intervals=_filter_intervals,\n\u001b[1;32m   2493\u001b[0m                                     _drop_cols=_drop_cols, _drop_rows=_drop_rows)\n",
      "\u001b[0;32m<decorator-gen-1219>\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, output, overwrite, stage_locally, _codec_spec, _partitions)\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/matrixtable.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, output, overwrite, stage_locally, _codec_spec, _partitions)\u001b[0m\n\u001b[1;32m   2526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2527\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMatrixNativeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage_locally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_codec_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_partitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_partitions_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2528\u001b[0;31m         \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMatrixWrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0m_Show\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mjir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_value_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mtimings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n\u001b[1;32m     39\u001b[0m                              \u001b[0;34m'Hail version: %s\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                              'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n",
      "\u001b[0;31mFatalError\u001b[0m: RemoteException: File /Sanger_cohorts_family_stats_gnomad_AF.mt/rows/rows/parts/part-06253-0-6253-3-061ac548-88a3-fba3-3556-d350a102eb9a could only be written to 0 of the 1 minReplication nodes. There are 25 datanode(s) running and 25 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\n\n\nJava stack trace:\njava.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:19)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:17)\n\tat is.hail.utils.package$.using(package.scala:600)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:17)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 6253 in stage 0.0 failed 4 times, most recent failure: Lost task 6253.3 in stage 0.0 (TID 6351, 192.168.252.152, executor 5): org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /Sanger_cohorts_family_stats_gnomad_AF.mt/rows/rows/parts/part-06253-0-6253-3-061ac548-88a3-fba3-3556-d350a102eb9a could only be written to 0 of the 1 minReplication nodes. There are 25 datanode(s) running and 25 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1491)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1388)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\n\tat com.sun.proxy.$Proxy21.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy22.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)\n\tat org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1866)\n\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:163)\n\tat is.hail.rvd.RVD.writeRowsSplit(RVD.scala:918)\n\tat is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:224)\n\tat is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:41)\n\tat is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:25)\n\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:653)\n\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:19)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:17)\n\tat is.hail.utils.package$.using(package.scala:600)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:17)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\norg.apache.hadoop.ipc.RemoteException: File /Sanger_cohorts_family_stats_gnomad_AF.mt/rows/rows/parts/part-06253-0-6253-3-061ac548-88a3-fba3-3556-d350a102eb9a could only be written to 0 of the 1 minReplication nodes. There are 25 datanode(s) running and 25 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1491)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1388)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\n\tat com.sun.proxy.$Proxy21.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy22.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)\n\tat org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1866)\n\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)\n\n\n\n\n\nHail version: 0.2.41-b8144dba46e6\nError summary: RemoteException: File /Sanger_cohorts_family_stats_gnomad_AF.mt/rows/rows/parts/part-06253-0-6253-3-061ac548-88a3-fba3-3556-d350a102eb9a could only be written to 0 of the 1 minReplication nodes. There are 25 datanode(s) running and 25 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\n"
     ]
    }
   ],
   "source": [
    "mt=mt.checkpoint(f'{tmp_dir}/Sanger_cohorts_family_stats_gnomad_AF.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-23 15:50:45 Hail: WARN: entries(): Resulting entries table is sorted by '(row_key, col_key)'.\n",
      "    To preserve row-major matrix table order, first unkey columns with 'key_cols_by()'\n"
     ]
    }
   ],
   "source": [
    "de_novo_table = hl.de_novo(\n",
    "        mt, pedigree, mt.gnomad_maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a82c3cea1fc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m         'locus', 'alleles').collect_by_key('de_novo_data')\n\u001b[1;32m      3\u001b[0m de_novo_table.write(\n\u001b[0;32m----> 4\u001b[0;31m         f'{tmp_dir}/Sanger_cohort_denovo_table.ht', overwrite=True)\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-1085>\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, output, overwrite, stage_locally, _codec_spec)\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/table.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, output, overwrite, stage_locally, _codec_spec)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \"\"\"\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTableWrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTableNativeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage_locally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_codec_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mjir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_value_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mtimings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "de_novo_table = de_novo_table.key_by(\n",
    "        'locus', 'alleles').collect_by_key('de_novo_data')\n",
    "de_novo_table.write(\n",
    "        f'{tmp_dir}/Sanger_cohort_denovo_table.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnomad_sites=hl.read_table('s3a://intervalwgs-qc/gnomad.genomes.r2.1.1.sites.liftover_grch38.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'rf': struct {\n",
      "        variants_by_type: dict<str, int32>, \n",
      "        feature_medians: dict<str, struct {\n",
      "            variant_type: str, \n",
      "            n_alt_alleles: int32, \n",
      "            qd: float64, \n",
      "            pab_max: float64, \n",
      "            info_MQRankSum: float64, \n",
      "            info_SOR: float64, \n",
      "            info_InbreedingCoeff: float64, \n",
      "            info_ReadPosRankSum: float64, \n",
      "            info_FS: float64, \n",
      "            info_QD: float64, \n",
      "            info_MQ: float64, \n",
      "            info_DP: int32\n",
      "        }>, \n",
      "        test_intervals: array<interval<locus<GRCh37>>>, \n",
      "        test_results: array<struct {\n",
      "            rf_prediction: str, \n",
      "            rf_label: str, \n",
      "            n: int32\n",
      "        }>, \n",
      "        features_importance: dict<str, float64>, \n",
      "        features: array<str>, \n",
      "        vqsr_training: bool, \n",
      "        no_transmitted_singletons: bool, \n",
      "        adj: bool, \n",
      "        rf_hash: str, \n",
      "        rf_snv_cutoff: struct {\n",
      "            bin: int32, \n",
      "            min_score: float64\n",
      "        }, \n",
      "        rf_indel_cutoff: struct {\n",
      "            bin: int32, \n",
      "            min_score: float64\n",
      "        }\n",
      "    } \n",
      "    'freq_meta': array<dict<str, str>> \n",
      "    'freq_index_dict': dict<str, int32> \n",
      "    'popmax_index_dict': dict<str, int32> \n",
      "    'age_index_dict': dict<str, int32> \n",
      "    'faf_index_dict': dict<str, int32> \n",
      "    'age_distribution': array<int32> \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'ReverseComplementedAlleles': bool \n",
      "    'SwappedAlleles': bool \n",
      "    'original_locus': locus<GRCh37> \n",
      "    'original_alleles': array<str> \n",
      "    'freq': array<struct {\n",
      "        AC: int32, \n",
      "        AF: float64, \n",
      "        AN: int32, \n",
      "        homozygote_count: int32\n",
      "    }> \n",
      "    'age_hist_het': array<struct {\n",
      "        bin_edges: array<float64>, \n",
      "        bin_freq: array<int64>, \n",
      "        n_smaller: int64, \n",
      "        n_larger: int64\n",
      "    }> \n",
      "    'age_hist_hom': array<struct {\n",
      "        bin_edges: array<float64>, \n",
      "        bin_freq: array<int64>, \n",
      "        n_smaller: int64, \n",
      "        n_larger: int64\n",
      "    }> \n",
      "    'popmax': array<struct {\n",
      "        AC: int32, \n",
      "        AF: float64, \n",
      "        AN: int32, \n",
      "        homozygote_count: int32, \n",
      "        pop: str\n",
      "    }> \n",
      "    'faf': array<struct {\n",
      "        meta: dict<str, str>, \n",
      "        faf95: float64, \n",
      "        faf99: float64\n",
      "    }> \n",
      "    'lcr': bool \n",
      "    'decoy': bool \n",
      "    'segdup': bool \n",
      "    'nonpar': bool \n",
      "    'variant_type': str \n",
      "    'allele_type': str \n",
      "    'n_alt_alleles': int32 \n",
      "    'was_mixed': bool \n",
      "    'has_star': bool \n",
      "    'qd': float64 \n",
      "    'pab_max': float64 \n",
      "    'info_MQRankSum': float64 \n",
      "    'info_SOR': float64 \n",
      "    'info_InbreedingCoeff': float64 \n",
      "    'info_ReadPosRankSum': float64 \n",
      "    'info_FS': float64 \n",
      "    'info_QD': float64 \n",
      "    'info_MQ': float64 \n",
      "    'info_DP': int32 \n",
      "    'transmitted_singleton': bool \n",
      "    'fail_hard_filters': bool \n",
      "    'info_POSITIVE_TRAIN_SITE': bool \n",
      "    'info_NEGATIVE_TRAIN_SITE': bool \n",
      "    'omni': bool \n",
      "    'mills': bool \n",
      "    'tp': bool \n",
      "    'rf_train': bool \n",
      "    'rf_label': str \n",
      "    'rf_probability': float64 \n",
      "    'rank': int64 \n",
      "    'was_split': bool \n",
      "    'singleton': bool \n",
      "    '_score': float64 \n",
      "    '_singleton': bool \n",
      "    'biallelic_rank': int64 \n",
      "    'singleton_rank': int64 \n",
      "    'n_nonref': int32 \n",
      "    'score': float64 \n",
      "    'adj_biallelic_singleton_rank': int64 \n",
      "    'adj_rank': int64 \n",
      "    'adj_biallelic_rank': int64 \n",
      "    'adj_singleton_rank': int64 \n",
      "    'biallelic_singleton_rank': int64 \n",
      "    'filters': set<str> \n",
      "    'gq_hist_alt': struct {\n",
      "        bin_edges: array<float64>, \n",
      "        bin_freq: array<int64>, \n",
      "        n_smaller: int64, \n",
      "        n_larger: int64\n",
      "    } \n",
      "    'gq_hist_all': struct {\n",
      "        bin_edges: array<float64>, \n",
      "        bin_freq: array<int64>, \n",
      "        n_smaller: int64, \n",
      "        n_larger: int64\n",
      "    } \n",
      "    'dp_hist_alt': struct {\n",
      "        bin_edges: array<float64>, \n",
      "        bin_freq: array<int64>, \n",
      "        n_smaller: int64, \n",
      "        n_larger: int64\n",
      "    } \n",
      "    'dp_hist_all': struct {\n",
      "        bin_edges: array<float64>, \n",
      "        bin_freq: array<int64>, \n",
      "        n_smaller: int64, \n",
      "        n_larger: int64\n",
      "    } \n",
      "    'ab_hist_alt': struct {\n",
      "        bin_edges: array<float64>, \n",
      "        bin_freq: array<int64>, \n",
      "        n_smaller: int64, \n",
      "        n_larger: int64\n",
      "    } \n",
      "    'qual': float64 \n",
      "    'vep': struct {\n",
      "        assembly_name: str, \n",
      "        allele_string: str, \n",
      "        ancestral: str, \n",
      "        colocated_variants: array<struct {\n",
      "            aa_allele: str, \n",
      "            aa_maf: float64, \n",
      "            afr_allele: str, \n",
      "            afr_maf: float64, \n",
      "            allele_string: str, \n",
      "            amr_allele: str, \n",
      "            amr_maf: float64, \n",
      "            clin_sig: array<str>, \n",
      "            end: int32, \n",
      "            eas_allele: str, \n",
      "            eas_maf: float64, \n",
      "            ea_allele: str, \n",
      "            ea_maf: float64, \n",
      "            eur_allele: str, \n",
      "            eur_maf: float64, \n",
      "            exac_adj_allele: str, \n",
      "            exac_adj_maf: float64, \n",
      "            exac_allele: str, \n",
      "            exac_afr_allele: str, \n",
      "            exac_afr_maf: float64, \n",
      "            exac_amr_allele: str, \n",
      "            exac_amr_maf: float64, \n",
      "            exac_eas_allele: str, \n",
      "            exac_eas_maf: float64, \n",
      "            exac_fin_allele: str, \n",
      "            exac_fin_maf: float64, \n",
      "            exac_maf: float64, \n",
      "            exac_nfe_allele: str, \n",
      "            exac_nfe_maf: float64, \n",
      "            exac_oth_allele: str, \n",
      "            exac_oth_maf: float64, \n",
      "            exac_sas_allele: str, \n",
      "            exac_sas_maf: float64, \n",
      "            id: str, \n",
      "            minor_allele: str, \n",
      "            minor_allele_freq: float64, \n",
      "            phenotype_or_disease: int32, \n",
      "            pubmed: array<int32>, \n",
      "            sas_allele: str, \n",
      "            sas_maf: float64, \n",
      "            somatic: int32, \n",
      "            start: int32, \n",
      "            strand: int32\n",
      "        }>, \n",
      "        context: str, \n",
      "        end: int32, \n",
      "        id: str, \n",
      "        input: str, \n",
      "        intergenic_consequences: array<struct {\n",
      "            allele_num: int32, \n",
      "            consequence_terms: array<str>, \n",
      "            impact: str, \n",
      "            minimised: int32, \n",
      "            variant_allele: str\n",
      "        }>, \n",
      "        most_severe_consequence: str, \n",
      "        motif_feature_consequences: array<struct {\n",
      "            allele_num: int32, \n",
      "            consequence_terms: array<str>, \n",
      "            high_inf_pos: str, \n",
      "            impact: str, \n",
      "            minimised: int32, \n",
      "            motif_feature_id: str, \n",
      "            motif_name: str, \n",
      "            motif_pos: int32, \n",
      "            motif_score_change: float64, \n",
      "            strand: int32, \n",
      "            variant_allele: str\n",
      "        }>, \n",
      "        regulatory_feature_consequences: array<struct {\n",
      "            allele_num: int32, \n",
      "            biotype: str, \n",
      "            consequence_terms: array<str>, \n",
      "            impact: str, \n",
      "            minimised: int32, \n",
      "            regulatory_feature_id: str, \n",
      "            variant_allele: str\n",
      "        }>, \n",
      "        seq_region_name: str, \n",
      "        start: int32, \n",
      "        strand: int32, \n",
      "        transcript_consequences: array<struct {\n",
      "            allele_num: int32, \n",
      "            amino_acids: str, \n",
      "            biotype: str, \n",
      "            canonical: int32, \n",
      "            ccds: str, \n",
      "            cdna_start: int32, \n",
      "            cdna_end: int32, \n",
      "            cds_end: int32, \n",
      "            cds_start: int32, \n",
      "            codons: str, \n",
      "            consequence_terms: array<str>, \n",
      "            distance: int32, \n",
      "            domains: array<struct {\n",
      "                db: str, \n",
      "                name: str\n",
      "            }>, \n",
      "            exon: str, \n",
      "            gene_id: str, \n",
      "            gene_pheno: int32, \n",
      "            gene_symbol: str, \n",
      "            gene_symbol_source: str, \n",
      "            hgnc_id: str, \n",
      "            hgvsc: str, \n",
      "            hgvsp: str, \n",
      "            hgvs_offset: int32, \n",
      "            impact: str, \n",
      "            intron: str, \n",
      "            lof: str, \n",
      "            lof_flags: str, \n",
      "            lof_filter: str, \n",
      "            lof_info: str, \n",
      "            minimised: int32, \n",
      "            polyphen_prediction: str, \n",
      "            polyphen_score: float64, \n",
      "            protein_end: int32, \n",
      "            protein_start: int32, \n",
      "            protein_id: str, \n",
      "            sift_prediction: str, \n",
      "            sift_score: float64, \n",
      "            strand: int32, \n",
      "            swissprot: str, \n",
      "            transcript_id: str, \n",
      "            trembl: str, \n",
      "            uniparc: str, \n",
      "            variant_allele: str\n",
      "        }>, \n",
      "        variant_class: str\n",
      "    } \n",
      "    'allele_info': struct {\n",
      "        BaseQRankSum: float64, \n",
      "        ClippingRankSum: float64, \n",
      "        DB: bool, \n",
      "        DP: int32, \n",
      "        DS: bool, \n",
      "        END: int32, \n",
      "        FS: float64, \n",
      "        HaplotypeScore: float64, \n",
      "        InbreedingCoeff: float64, \n",
      "        MQ: float64, \n",
      "        MQ0: int32, \n",
      "        MQRankSum: float64, \n",
      "        NEGATIVE_TRAIN_SITE: bool, \n",
      "        POSITIVE_TRAIN_SITE: bool, \n",
      "        QD: float64, \n",
      "        RAW_MQ: float64, \n",
      "        ReadPosRankSum: float64, \n",
      "        SOR: float64, \n",
      "        VQSLOD: float64, \n",
      "        culprit: str\n",
      "    } \n",
      "    'rsid': str \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gnomad_sites.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td><expr></td></tr>\n",
       "<tr><td>locus&lt;GRCh38&gt;</td><td>array&lt;str&gt;</td><td>array&lt;float64&gt;</td></tr>\n",
       "</thead><tbody><tr><td>chr1:10067</td><td>[&quot;T&quot;,&quot;TAACCCTAACCCTAACCCTAACCCTAACCCT...</td><td>[3.80e-05,9.55e-05,0.00e+00,3.75e-04,...</td></tr>\n",
       "<tr><td>chr1:10108</td><td>[&quot;CAACCCT&quot;,&quot;C&quot;]</td><td>[1.01e-03,6.49e-05,0.00e+00,2.27e-02,...</td></tr>\n",
       "<tr><td>chr1:10109</td><td>[&quot;AACCCT&quot;,&quot;A&quot;]</td><td>[6.42e-02,1.01e-03,5.13e-02,5.88e-02,...</td></tr>\n",
       "<tr><td>chr1:10114</td><td>[&quot;T&quot;,&quot;C&quot;]</td><td>[0.00e+00,2.56e-04,0.00e+00,0.00e+00,...</td></tr>\n",
       "<tr><td>chr1:10114</td><td>[&quot;TAACCCTAACCCTAACCCTAACCCTAACCCTAACC...</td><td>[1.14e-04,3.21e-05,0.00e+00,0.00e+00,...</td></tr>\n",
       "<tr><td>chr1:10119</td><td>[&quot;CT&quot;,&quot;C&quot;]</td><td>[0.00e+00,1.28e-04,0.00e+00,0.00e+00,...</td></tr>\n",
       "<tr><td>chr1:10120</td><td>[&quot;T&quot;,&quot;C&quot;]</td><td>[0.00e+00,2.89e-04,0.00e+00,0.00e+00,...</td></tr>\n",
       "<tr><td>chr1:10128</td><td>[&quot;ACCCTAACCCTAACCCTAAC&quot;,&quot;A&quot;]</td><td>[3.95e-05,9.66e-05,8.18e-05,0.00e+00,...</td></tr>\n",
       "<tr><td>chr1:10131</td><td>[&quot;CT&quot;,&quot;C&quot;]</td><td>[3.66e-05,2.25e-04,0.00e+00,3.38e-04,...</td></tr>\n",
       "<tr><td>chr1:10132</td><td>[&quot;TAACCC&quot;,&quot;T&quot;]</td><td>[0.00e+00,6.44e-05,0.00e+00,0.00e+00,...</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+------------------------------------------+------------------------------------------+\n",
       "| locus         | alleles                                  | <expr>                                   |\n",
       "+---------------+------------------------------------------+------------------------------------------+\n",
       "| locus<GRCh38> | array<str>                               | array<float64>                           |\n",
       "+---------------+------------------------------------------+------------------------------------------+\n",
       "| chr1:10067    | [\"T\",\"TAACCCTAACCCTAACCCTAACCCTAACCCT... | [3.80e-05,9.55e-05,0.00e+00,3.75e-04,... |\n",
       "| chr1:10108    | [\"CAACCCT\",\"C\"]                          | [1.01e-03,6.49e-05,0.00e+00,2.27e-02,... |\n",
       "| chr1:10109    | [\"AACCCT\",\"A\"]                           | [6.42e-02,1.01e-03,5.13e-02,5.88e-02,... |\n",
       "| chr1:10114    | [\"T\",\"C\"]                                | [0.00e+00,2.56e-04,0.00e+00,0.00e+00,... |\n",
       "| chr1:10114    | [\"TAACCCTAACCCTAACCCTAACCCTAACCCTAACC... | [1.14e-04,3.21e-05,0.00e+00,0.00e+00,... |\n",
       "| chr1:10119    | [\"CT\",\"C\"]                               | [0.00e+00,1.28e-04,0.00e+00,0.00e+00,... |\n",
       "| chr1:10120    | [\"T\",\"C\"]                                | [0.00e+00,2.89e-04,0.00e+00,0.00e+00,... |\n",
       "| chr1:10128    | [\"ACCCTAACCCTAACCCTAAC\",\"A\"]             | [3.95e-05,9.66e-05,8.18e-05,0.00e+00,... |\n",
       "| chr1:10131    | [\"CT\",\"C\"]                               | [3.66e-05,2.25e-04,0.00e+00,3.38e-04,... |\n",
       "| chr1:10132    | [\"TAACCC\",\"T\"]                           | [0.00e+00,6.44e-05,0.00e+00,0.00e+00,... |\n",
       "+---------------+------------------------------------------+------------------------------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnomad_sites.freq.AF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " mt = hl.read_matrix_table(\n",
    "        f'{temp_dir}/ddd-elgh-ukbb/variant_qc/Sanger_cohorts_family_stats.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hash=\"91b132aa\"\n",
    "ht_RF = hl.read_table(\n",
    "        f'{temp_dir}/ddd-elgh-ukbb/variant_qc/models/{run_hash}/rf_result_sanger_cohorts_new.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht=hl.read_table(\n",
    "        f'{temp_dir}/ddd-elgh-ukbb/variant_qc/Sanger_cohort_denovo_table.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td>de_novo_data</td></tr>\n",
       "<tr><td>locus&lt;GRCh38&gt;</td><td>array&lt;str&gt;</td><td>array&lt;struct{id: str, prior: float64, proband: struct{s: str}, father: struct{s: str}, mother: struct{s: str}, proband_entry: struct{AD: array&lt;int32&gt;, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array&lt;int32&gt;, PS: int32, RGQ: int32, SB: array&lt;int32&gt;}, father_entry: struct{AD: array&lt;int32&gt;, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array&lt;int32&gt;, PS: int32, RGQ: int32, SB: array&lt;int32&gt;}, mother_entry: struct{AD: array&lt;int32&gt;, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array&lt;int32&gt;, PS: int32, RGQ: int32, SB: array&lt;int32&gt;}, is_female: bool, p_de_novo: float64, confidence: str}&gt;</td></tr>\n",
       "</thead><tbody><tr><td>chr1:16959</td><td>[&quot;G&quot;,&quot;C&quot;]</td><td>[(&quot;EGAN00001315776&quot;,3.36e-05,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17375</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>[(&quot;EGAN00001343225&quot;,1.34e-02,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17407</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[(&quot;EGAN00001343225&quot;,5.75e-03,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17452</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>[(&quot;EGAN00001313169&quot;,3.21e-03,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17487</td><td>[&quot;C&quot;,&quot;A&quot;]</td><td>[(&quot;EGAN00001315786&quot;,3.33e-06,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17512</td><td>[&quot;C&quot;,&quot;G&quot;]</td><td>[(&quot;EGAN00001315333&quot;,4.13e-04,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17519</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>[(&quot;EGAN00001324367&quot;,2.20e-02,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:63628</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[(&quot;EGAN00001342824&quot;,3.33e-06,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:133383</td><td>[&quot;A&quot;,&quot;ACT&quot;]</td><td>[(&quot;EGAN00001342875&quot;,3.33e-06,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:135166</td><td>[&quot;T&quot;,&quot;TGAGGCC&quot;]</td><td>[(&quot;EGAN00001319138&quot;,2.01e-05,(&quot;EGAN00...</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+-----------------+------------------------------------------+\n",
       "| locus         | alleles         | de_novo_data                             |\n",
       "+---------------+-----------------+------------------------------------------+\n",
       "| locus<GRCh38> | array<str>      | array<struct{id: str, prior: float64,... |\n",
       "+---------------+-----------------+------------------------------------------+\n",
       "| chr1:16959    | [\"G\",\"C\"]       | [(\"EGAN00001315776\",3.36e-05,(\"EGAN00... |\n",
       "| chr1:17375    | [\"A\",\"G\"]       | [(\"EGAN00001343225\",1.34e-02,(\"EGAN00... |\n",
       "| chr1:17407    | [\"G\",\"A\"]       | [(\"EGAN00001343225\",5.75e-03,(\"EGAN00... |\n",
       "| chr1:17452    | [\"C\",\"T\"]       | [(\"EGAN00001313169\",3.21e-03,(\"EGAN00... |\n",
       "| chr1:17487    | [\"C\",\"A\"]       | [(\"EGAN00001315786\",3.33e-06,(\"EGAN00... |\n",
       "| chr1:17512    | [\"C\",\"G\"]       | [(\"EGAN00001315333\",4.13e-04,(\"EGAN00... |\n",
       "| chr1:17519    | [\"G\",\"T\"]       | [(\"EGAN00001324367\",2.20e-02,(\"EGAN00... |\n",
       "| chr1:63628    | [\"G\",\"A\"]       | [(\"EGAN00001342824\",3.33e-06,(\"EGAN00... |\n",
       "| chr1:133383   | [\"A\",\"ACT\"]     | [(\"EGAN00001342875\",3.33e-06,(\"EGAN00... |\n",
       "| chr1:135166   | [\"T\",\"TGAGGCC\"] | [(\"EGAN00001319138\",2.01e-05,(\"EGAN00... |\n",
       "+---------------+-----------------+------------------------------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ht.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#annotate with de novo table\n",
    "ht=hl.read_table(\n",
    "        f'{temp_dir}/ddd-elgh-ukbb/variant_qc/Sanger_cohort_denovo_table.ht')\n",
    "ht_RF=ht_RF.annotate(de_novo_data=ht[ht_RF.key].de_novo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotate with family stats\n",
    "ht=hl.read_table(\n",
    "        f'{temp_dir}/ddd-elgh-ukbb/variant_qc/Sanger_cohorts_family_stats.ht')\n",
    "ht_RF=ht_RF.annotate(family_stats=ht[ht_RF.key].family_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 17:17:51 Hail: INFO: wrote table with 13669739 rows in 500 partitions to hdfs://spark-master:9820//variant_qc/models/91b132aa/91b132aa_rf_result_sanger_cohorts_DENOVO_family_stats.ht\n"
     ]
    }
   ],
   "source": [
    "ht_RF.write(f\"{tmp_dir}/variant_qc/models/{run_hash}/{run_hash}_rf_result_sanger_cohorts_DENOVO_family_stats.ht\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hash=\"91b132aa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hash=\"91b132aa\"\n",
    "ht=hl.read_table(f'{temp_dir}/ddd-elgh-ukbb//variant_qc/models/{run_hash}/{run_hash}_rf_result_sanger_cohorts_DENOVO_family_stats.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'feature_medians': dict<tuple (\n",
      "        str\n",
      "    ), struct {\n",
      "        a_index: int32, \n",
      "        n_alt_alleles: int32, \n",
      "        QD: float64, \n",
      "        MQRankSum: float64, \n",
      "        SOR: float64, \n",
      "        ReadPosRankSum: float64, \n",
      "        FS: float64, \n",
      "        DP: int32\n",
      "    }> \n",
      "    'variants_by_strata': dict<tuple (\n",
      "        str\n",
      "    ), int64> \n",
      "    'features_importance': dict<str, float64> \n",
      "    'features': array<str> \n",
      "    'test_results': array<struct {\n",
      "        rf_prediction: str, \n",
      "        rf_label: str, \n",
      "        n: int32\n",
      "    }> \n",
      "    'rf_hash': str \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'a_index': int32 \n",
      "    'was_split': bool \n",
      "    'InbreedingCoeff': float32 \n",
      "    'variant_type': str \n",
      "    'allele_type': str \n",
      "    'n_alt_alleles': int32 \n",
      "    'was_mixed': bool \n",
      "    'has_star': bool \n",
      "    'QD': float64 \n",
      "    'MQRankSum': float64 \n",
      "    'SOR': float64 \n",
      "    'ReadPosRankSum': float64 \n",
      "    'FS': float64 \n",
      "    'DP': int32 \n",
      "    'hapmap': bool \n",
      "    'omni': bool \n",
      "    'mills': bool \n",
      "    'kgp_phase1_hc': bool \n",
      "    'transmitted_singleton': bool \n",
      "    'fail_hard_filters': bool \n",
      "    'ac_raw': int64 \n",
      "    'feature_imputed': struct {\n",
      "        a_index: bool, \n",
      "        n_alt_alleles: bool, \n",
      "        QD: bool, \n",
      "        MQRankSum: bool, \n",
      "        SOR: bool, \n",
      "        ReadPosRankSum: bool, \n",
      "        FS: bool, \n",
      "        DP: bool\n",
      "    } \n",
      "    'tp': bool \n",
      "    'fp': bool \n",
      "    'rf_train': bool \n",
      "    'rf_label': str \n",
      "    'rf_test': bool \n",
      "    'rf_probability': dict<str, float64> \n",
      "    'rf_prediction': str \n",
      "    'de_novo_data': array<struct {\n",
      "        id: str, \n",
      "        prior: float64, \n",
      "        proband: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        father: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        mother: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        proband_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        father_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        mother_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        is_female: bool, \n",
      "        p_de_novo: float64, \n",
      "        confidence: str\n",
      "    }> \n",
      "    'family_stats': array<struct {\n",
      "        mendel: struct {\n",
      "            errors: int64\n",
      "        }, \n",
      "        tdt: struct {\n",
      "            t: int64, \n",
      "            u: int64, \n",
      "            chi_sq: float64, \n",
      "            p_value: float64\n",
      "        }, \n",
      "        unrelated_qc_callstats: struct {\n",
      "            AC: array<int32>, \n",
      "            AF: array<float64>, \n",
      "            AN: int32, \n",
      "            homozygote_count: array<int32>\n",
      "        }, \n",
      "        meta: dict<str, str>\n",
      "    }> \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13669739"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_validated=ht.filter(ht.de_novo_data[0].p_de_novo >0.99, keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90912"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht_validated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td><expr></td></tr>\n",
       "<tr><td>locus&lt;GRCh38&gt;</td><td>array&lt;str&gt;</td><td>int32</td></tr>\n",
       "</thead><tbody><tr><td>chr1:12938</td><td>[&quot;GCAAA&quot;,&quot;G&quot;]</td><td>2</td></tr>\n",
       "<tr><td>chr1:13024</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>0</td></tr>\n",
       "<tr><td>chr1:13087</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>3</td></tr>\n",
       "<tr><td>chr1:13116</td><td>[&quot;T&quot;,&quot;C&quot;]</td><td>2</td></tr>\n",
       "<tr><td>chr1:13130</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>2</td></tr>\n",
       "<tr><td>chr1:13151</td><td>[&quot;G&quot;,&quot;C&quot;]</td><td>2</td></tr>\n",
       "<tr><td>chr1:13164</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>2</td></tr>\n",
       "<tr><td>chr1:13176</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>1</td></tr>\n",
       "<tr><td>chr1:13198</td><td>[&quot;C&quot;,&quot;A&quot;]</td><td>1</td></tr>\n",
       "<tr><td>chr1:13216</td><td>[&quot;C&quot;,&quot;G&quot;]</td><td>1</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+---------------+--------+\n",
       "| locus         | alleles       | <expr> |\n",
       "+---------------+---------------+--------+\n",
       "| locus<GRCh38> | array<str>    |  int32 |\n",
       "+---------------+---------------+--------+\n",
       "| chr1:12938    | [\"GCAAA\",\"G\"] |      2 |\n",
       "| chr1:13024    | [\"G\",\"A\"]     |      0 |\n",
       "| chr1:13087    | [\"A\",\"G\"]     |      3 |\n",
       "| chr1:13116    | [\"T\",\"C\"]     |      2 |\n",
       "| chr1:13130    | [\"C\",\"T\"]     |      2 |\n",
       "| chr1:13151    | [\"G\",\"C\"]     |      2 |\n",
       "| chr1:13164    | [\"G\",\"A\"]     |      2 |\n",
       "| chr1:13176    | [\"G\",\"T\"]     |      1 |\n",
       "| chr1:13198    | [\"C\",\"A\"]     |      1 |\n",
       "| chr1:13216    | [\"C\",\"G\"]     |      1 |\n",
       "+---------------+---------------+--------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ht.family_stats.unrelated_qc_callstats.AC[0][1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td><expr></td></tr>\n",
       "<tr><td>locus&lt;GRCh38&gt;</td><td>array&lt;str&gt;</td><td>array&lt;struct{errors: int64}&gt;</td></tr>\n",
       "</thead><tbody><tr><td>chr1:12938</td><td>[&quot;GCAAA&quot;,&quot;G&quot;]</td><td>[(0)]</td></tr>\n",
       "<tr><td>chr1:13024</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[(1)]</td></tr>\n",
       "<tr><td>chr1:13087</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>[(1)]</td></tr>\n",
       "<tr><td>chr1:13116</td><td>[&quot;T&quot;,&quot;C&quot;]</td><td>[(0)]</td></tr>\n",
       "<tr><td>chr1:13130</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>[(0)]</td></tr>\n",
       "<tr><td>chr1:13151</td><td>[&quot;G&quot;,&quot;C&quot;]</td><td>[(0)]</td></tr>\n",
       "<tr><td>chr1:13164</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[(0)]</td></tr>\n",
       "<tr><td>chr1:13176</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>[(0)]</td></tr>\n",
       "<tr><td>chr1:13198</td><td>[&quot;C&quot;,&quot;A&quot;]</td><td>[(0)]</td></tr>\n",
       "<tr><td>chr1:13216</td><td>[&quot;C&quot;,&quot;G&quot;]</td><td>[(1)]</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+---------------+------------------------------+\n",
       "| locus         | alleles       | <expr>                       |\n",
       "+---------------+---------------+------------------------------+\n",
       "| locus<GRCh38> | array<str>    | array<struct{errors: int64}> |\n",
       "+---------------+---------------+------------------------------+\n",
       "| chr1:12938    | [\"GCAAA\",\"G\"] | [(0)]                        |\n",
       "| chr1:13024    | [\"G\",\"A\"]     | [(1)]                        |\n",
       "| chr1:13087    | [\"A\",\"G\"]     | [(1)]                        |\n",
       "| chr1:13116    | [\"T\",\"C\"]     | [(0)]                        |\n",
       "| chr1:13130    | [\"C\",\"T\"]     | [(0)]                        |\n",
       "| chr1:13151    | [\"G\",\"C\"]     | [(0)]                        |\n",
       "| chr1:13164    | [\"G\",\"A\"]     | [(0)]                        |\n",
       "| chr1:13176    | [\"G\",\"T\"]     | [(0)]                        |\n",
       "| chr1:13198    | [\"C\",\"A\"]     | [(0)]                        |\n",
       "| chr1:13216    | [\"C\",\"G\"]     | [(1)]                        |\n",
       "+---------------+---------------+------------------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ht.family_stats.mendel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td><expr></td></tr>\n",
       "<tr><td>locus&lt;GRCh38&gt;</td><td>array&lt;str&gt;</td><td>int64</td></tr>\n",
       "</thead><tbody><tr><td>chr1:12938</td><td>[&quot;GCAAA&quot;,&quot;G&quot;]</td><td>0</td></tr>\n",
       "<tr><td>chr1:13024</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>1</td></tr>\n",
       "<tr><td>chr1:13087</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>1</td></tr>\n",
       "<tr><td>chr1:13116</td><td>[&quot;T&quot;,&quot;C&quot;]</td><td>0</td></tr>\n",
       "<tr><td>chr1:13130</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>0</td></tr>\n",
       "<tr><td>chr1:13151</td><td>[&quot;G&quot;,&quot;C&quot;]</td><td>0</td></tr>\n",
       "<tr><td>chr1:13164</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>0</td></tr>\n",
       "<tr><td>chr1:13176</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>0</td></tr>\n",
       "<tr><td>chr1:13198</td><td>[&quot;C&quot;,&quot;A&quot;]</td><td>0</td></tr>\n",
       "<tr><td>chr1:13216</td><td>[&quot;C&quot;,&quot;G&quot;]</td><td>1</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+---------------+--------+\n",
       "| locus         | alleles       | <expr> |\n",
       "+---------------+---------------+--------+\n",
       "| locus<GRCh38> | array<str>    |  int64 |\n",
       "+---------------+---------------+--------+\n",
       "| chr1:12938    | [\"GCAAA\",\"G\"] |      0 |\n",
       "| chr1:13024    | [\"G\",\"A\"]     |      1 |\n",
       "| chr1:13087    | [\"A\",\"G\"]     |      1 |\n",
       "| chr1:13116    | [\"T\",\"C\"]     |      0 |\n",
       "| chr1:13130    | [\"C\",\"T\"]     |      0 |\n",
       "| chr1:13151    | [\"G\",\"C\"]     |      0 |\n",
       "| chr1:13164    | [\"G\",\"A\"]     |      0 |\n",
       "| chr1:13176    | [\"G\",\"T\"]     |      0 |\n",
       "| chr1:13198    | [\"C\",\"A\"]     |      0 |\n",
       "| chr1:13216    | [\"C\",\"G\"]     |      1 |\n",
       "+---------------+---------------+--------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ht.family_stats.mendel[0].errors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td><expr>.t</td><td><expr>.u</td><td><expr>.chi_sq</td><td><expr>.p_value</td></tr>\n",
       "<tr><td>locus&lt;GRCh38&gt;</td><td>array&lt;str&gt;</td><td>int64</td><td>int64</td><td>float64</td><td>float64</td></tr>\n",
       "</thead><tbody><tr><td>chr1:12938</td><td>[&quot;GCAAA&quot;,&quot;G&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "<tr><td>chr1:13024</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "<tr><td>chr1:13087</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "<tr><td>chr1:13116</td><td>[&quot;T&quot;,&quot;C&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "<tr><td>chr1:13130</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "<tr><td>chr1:13151</td><td>[&quot;G&quot;,&quot;C&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "<tr><td>chr1:13164</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "<tr><td>chr1:13176</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "<tr><td>chr1:13198</td><td>[&quot;C&quot;,&quot;A&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "<tr><td>chr1:13216</td><td>[&quot;C&quot;,&quot;G&quot;]</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+---------------+----------+----------+---------------+----------------+\n",
       "| locus         | alleles       | <expr>.t | <expr>.u | <expr>.chi_sq | <expr>.p_value |\n",
       "+---------------+---------------+----------+----------+---------------+----------------+\n",
       "| locus<GRCh38> | array<str>    |    int64 |    int64 |       float64 |        float64 |\n",
       "+---------------+---------------+----------+----------+---------------+----------------+\n",
       "| chr1:12938    | [\"GCAAA\",\"G\"] |       NA |       NA |            NA |             NA |\n",
       "| chr1:13024    | [\"G\",\"A\"]     |       NA |       NA |            NA |             NA |\n",
       "| chr1:13087    | [\"A\",\"G\"]     |       NA |       NA |            NA |             NA |\n",
       "| chr1:13116    | [\"T\",\"C\"]     |       NA |       NA |            NA |             NA |\n",
       "| chr1:13130    | [\"C\",\"T\"]     |       NA |       NA |            NA |             NA |\n",
       "| chr1:13151    | [\"G\",\"C\"]     |       NA |       NA |            NA |             NA |\n",
       "| chr1:13164    | [\"G\",\"A\"]     |       NA |       NA |            NA |             NA |\n",
       "| chr1:13176    | [\"G\",\"T\"]     |       NA |       NA |            NA |             NA |\n",
       "| chr1:13198    | [\"C\",\"A\"]     |       NA |       NA |            NA |             NA |\n",
       "| chr1:13216    | [\"C\",\"G\"]     |       NA |       NA |            NA |             NA |\n",
       "+---------------+---------------+----------+----------+---------------+----------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ht.family_stats.tdt[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht=hl.read_table(\n",
    "        f'{temp_dir}/ddd-elgh-ukbb/variant_qc/Sanger_cohorts_family_stats.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'family_stats': array<struct {\n",
      "        mendel: struct {\n",
      "            errors: int64\n",
      "        }, \n",
      "        tdt: struct {\n",
      "            t: int64, \n",
      "            u: int64, \n",
      "            chi_sq: float64, \n",
      "            p_value: float64\n",
      "        }, \n",
      "        unrelated_qc_callstats: struct {\n",
      "            AC: array<int32>, \n",
      "            AF: array<float64>, \n",
      "            AN: int32, \n",
      "            homozygote_count: array<int32>\n",
      "        }, \n",
      "        meta: dict<str, str>\n",
      "    }> \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht=hl.read_table(\n",
    "        f'{temp_dir}/ddd-elgh-ukbb/variant_qc/Sanger_cohort_denovo_table.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'de_novo_data': array<struct {\n",
      "        id: str, \n",
      "        prior: float64, \n",
      "        proband: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        father: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        mother: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        proband_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        father_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        mother_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        is_female: bool, \n",
      "        p_de_novo: float64, \n",
      "        confidence: str\n",
      "    }> \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td>de_novo_data</td></tr>\n",
       "<tr><td>locus&lt;GRCh38&gt;</td><td>array&lt;str&gt;</td><td>array&lt;struct{id: str, prior: float64, proband: struct{s: str}, father: struct{s: str}, mother: struct{s: str}, proband_entry: struct{AD: array&lt;int32&gt;, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array&lt;int32&gt;, PS: int32, RGQ: int32, SB: array&lt;int32&gt;}, father_entry: struct{AD: array&lt;int32&gt;, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array&lt;int32&gt;, PS: int32, RGQ: int32, SB: array&lt;int32&gt;}, mother_entry: struct{AD: array&lt;int32&gt;, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array&lt;int32&gt;, PS: int32, RGQ: int32, SB: array&lt;int32&gt;}, is_female: bool, p_de_novo: float64, confidence: str}&gt;</td></tr>\n",
       "</thead><tbody><tr><td>chr1:16959</td><td>[&quot;G&quot;,&quot;C&quot;]</td><td>[(&quot;EGAN00001315776&quot;,3.36e-05,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17375</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>[(&quot;EGAN00001343225&quot;,1.34e-02,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17407</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[(&quot;EGAN00001343225&quot;,5.75e-03,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17452</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>[(&quot;EGAN00001313169&quot;,3.21e-03,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17487</td><td>[&quot;C&quot;,&quot;A&quot;]</td><td>[(&quot;EGAN00001315786&quot;,3.33e-06,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17512</td><td>[&quot;C&quot;,&quot;G&quot;]</td><td>[(&quot;EGAN00001315333&quot;,4.13e-04,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17519</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>[(&quot;EGAN00001324367&quot;,2.20e-02,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:63628</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[(&quot;EGAN00001342824&quot;,3.33e-06,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:133383</td><td>[&quot;A&quot;,&quot;ACT&quot;]</td><td>[(&quot;EGAN00001342875&quot;,3.33e-06,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:135166</td><td>[&quot;T&quot;,&quot;TGAGGCC&quot;]</td><td>[(&quot;EGAN00001319138&quot;,2.01e-05,(&quot;EGAN00...</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+-----------------+------------------------------------------+\n",
       "| locus         | alleles         | de_novo_data                             |\n",
       "+---------------+-----------------+------------------------------------------+\n",
       "| locus<GRCh38> | array<str>      | array<struct{id: str, prior: float64,... |\n",
       "+---------------+-----------------+------------------------------------------+\n",
       "| chr1:16959    | [\"G\",\"C\"]       | [(\"EGAN00001315776\",3.36e-05,(\"EGAN00... |\n",
       "| chr1:17375    | [\"A\",\"G\"]       | [(\"EGAN00001343225\",1.34e-02,(\"EGAN00... |\n",
       "| chr1:17407    | [\"G\",\"A\"]       | [(\"EGAN00001343225\",5.75e-03,(\"EGAN00... |\n",
       "| chr1:17452    | [\"C\",\"T\"]       | [(\"EGAN00001313169\",3.21e-03,(\"EGAN00... |\n",
       "| chr1:17487    | [\"C\",\"A\"]       | [(\"EGAN00001315786\",3.33e-06,(\"EGAN00... |\n",
       "| chr1:17512    | [\"C\",\"G\"]       | [(\"EGAN00001315333\",4.13e-04,(\"EGAN00... |\n",
       "| chr1:17519    | [\"G\",\"T\"]       | [(\"EGAN00001324367\",2.20e-02,(\"EGAN00... |\n",
       "| chr1:63628    | [\"G\",\"A\"]       | [(\"EGAN00001342824\",3.33e-06,(\"EGAN00... |\n",
       "| chr1:133383   | [\"A\",\"ACT\"]     | [(\"EGAN00001342875\",3.33e-06,(\"EGAN00... |\n",
       "| chr1:135166   | [\"T\",\"TGAGGCC\"] | [(\"EGAN00001319138\",2.01e-05,(\"EGAN00... |\n",
       "+---------------+-----------------+------------------------------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ht.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td><expr></td></tr>\n",
       "<tr><td>locus&lt;GRCh38&gt;</td><td>array&lt;str&gt;</td><td>array&lt;float64&gt;</td></tr>\n",
       "</thead><tbody><tr><td>chr1:16959</td><td>[&quot;G&quot;,&quot;C&quot;]</td><td>[2.84e-01]</td></tr>\n",
       "<tr><td>chr1:17375</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>[5.05e-02]</td></tr>\n",
       "<tr><td>chr1:17407</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[1.54e-01]</td></tr>\n",
       "<tr><td>chr1:17452</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>[5.93e-01]</td></tr>\n",
       "<tr><td>chr1:17487</td><td>[&quot;C&quot;,&quot;A&quot;]</td><td>[6.12e-01]</td></tr>\n",
       "<tr><td>chr1:17512</td><td>[&quot;C&quot;,&quot;G&quot;]</td><td>[9.18e-02]</td></tr>\n",
       "<tr><td>chr1:17519</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>[1.60e-01,5.54e-01]</td></tr>\n",
       "<tr><td>chr1:63628</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[1.00e+00]</td></tr>\n",
       "<tr><td>chr1:133383</td><td>[&quot;A&quot;,&quot;ACT&quot;]</td><td>[5.00e-02]</td></tr>\n",
       "<tr><td>chr1:135166</td><td>[&quot;T&quot;,&quot;TGAGGCC&quot;]</td><td>[4.52e-01]</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+-----------------+---------------------+\n",
       "| locus         | alleles         | <expr>              |\n",
       "+---------------+-----------------+---------------------+\n",
       "| locus<GRCh38> | array<str>      | array<float64>      |\n",
       "+---------------+-----------------+---------------------+\n",
       "| chr1:16959    | [\"G\",\"C\"]       | [2.84e-01]          |\n",
       "| chr1:17375    | [\"A\",\"G\"]       | [5.05e-02]          |\n",
       "| chr1:17407    | [\"G\",\"A\"]       | [1.54e-01]          |\n",
       "| chr1:17452    | [\"C\",\"T\"]       | [5.93e-01]          |\n",
       "| chr1:17487    | [\"C\",\"A\"]       | [6.12e-01]          |\n",
       "| chr1:17512    | [\"C\",\"G\"]       | [9.18e-02]          |\n",
       "| chr1:17519    | [\"G\",\"T\"]       | [1.60e-01,5.54e-01] |\n",
       "| chr1:63628    | [\"G\",\"A\"]       | [1.00e+00]          |\n",
       "| chr1:133383   | [\"A\",\"ACT\"]     | [5.00e-02]          |\n",
       "| chr1:135166   | [\"T\",\"TGAGGCC\"] | [4.52e-01]          |\n",
       "+---------------+-----------------+---------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ht.de_novo_data.p_de_novo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hash=\"91b132aa\"\n",
    "ht_RF=hl.read_table(\n",
    "        f'{temp_dir}/ddd-elgh-ukbb/variant_qc/models/{run_hash}/rf_result_sanger_cohorts_new.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_RF=ht_RF.annotate(de_novo_data=ht[ht_RF.key].de_novo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 11:42:10 Hail: INFO: wrote table with 13669739 rows in 500 partitions to hdfs://spark-master:9820//variant_qc/models/91b132aa/91b132aa_rf_result_sanger_cohorts_DENOVO.ht\n"
     ]
    }
   ],
   "source": [
    "ht_RF.write(f\"{tmp_dir}/variant_qc/models/{run_hash}/{run_hash}_rf_result_sanger_cohorts_DENOVO.ht\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = hl.read_table(\n",
    "        f'{temp_dir}/ddd-elgh-ukbb/variant_qc/models/{run_hash}/{run_hash}_rf_result_sanger_cohorts_DENOVO.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'feature_medians': dict<tuple (\n",
      "        str\n",
      "    ), struct {\n",
      "        a_index: int32, \n",
      "        n_alt_alleles: int32, \n",
      "        QD: float64, \n",
      "        MQRankSum: float64, \n",
      "        SOR: float64, \n",
      "        ReadPosRankSum: float64, \n",
      "        FS: float64, \n",
      "        DP: int32\n",
      "    }> \n",
      "    'variants_by_strata': dict<tuple (\n",
      "        str\n",
      "    ), int64> \n",
      "    'features_importance': dict<str, float64> \n",
      "    'features': array<str> \n",
      "    'test_results': array<struct {\n",
      "        rf_prediction: str, \n",
      "        rf_label: str, \n",
      "        n: int32\n",
      "    }> \n",
      "    'rf_hash': str \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'a_index': int32 \n",
      "    'was_split': bool \n",
      "    'InbreedingCoeff': float32 \n",
      "    'variant_type': str \n",
      "    'allele_type': str \n",
      "    'n_alt_alleles': int32 \n",
      "    'was_mixed': bool \n",
      "    'has_star': bool \n",
      "    'QD': float64 \n",
      "    'MQRankSum': float64 \n",
      "    'SOR': float64 \n",
      "    'ReadPosRankSum': float64 \n",
      "    'FS': float64 \n",
      "    'DP': int32 \n",
      "    'hapmap': bool \n",
      "    'omni': bool \n",
      "    'mills': bool \n",
      "    'kgp_phase1_hc': bool \n",
      "    'transmitted_singleton': bool \n",
      "    'fail_hard_filters': bool \n",
      "    'ac_raw': int64 \n",
      "    'feature_imputed': struct {\n",
      "        a_index: bool, \n",
      "        n_alt_alleles: bool, \n",
      "        QD: bool, \n",
      "        MQRankSum: bool, \n",
      "        SOR: bool, \n",
      "        ReadPosRankSum: bool, \n",
      "        FS: bool, \n",
      "        DP: bool\n",
      "    } \n",
      "    'tp': bool \n",
      "    'fp': bool \n",
      "    'rf_train': bool \n",
      "    'rf_label': str \n",
      "    'rf_test': bool \n",
      "    'rf_probability': dict<str, float64> \n",
      "    'rf_prediction': str \n",
      "    'de_novo_data': array<struct {\n",
      "        id: str, \n",
      "        prior: float64, \n",
      "        proband: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        father: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        mother: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        proband_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        father_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        mother_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        is_female: bool, \n",
      "        p_de_novo: float64, \n",
      "        confidence: str\n",
      "    }> \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht=ht.annotate(high_quality_denovo=hl.is_defined(ht.de_novo_data.p_de_novo[0]> 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_rows(family_stats=ht[mt.row_key].family_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = f\"{temp_dir}/ddd-elgh-ukbb/variant_qc/DDD_trios.fam\"\n",
    "pedigree = hl.Pedigree.read(fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 16:09:03 Hail: WARN: entries(): Resulting entries table is sorted by '(row_key, col_key)'.\n",
      "    To preserve row-major matrix table order, first unkey columns with 'key_cols_by()'\n"
     ]
    }
   ],
   "source": [
    "de_novo_table = hl.de_novo(\n",
    "        mt, pedigree, mt.family_stats[0].unrelated_qc_callstats.AF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_novo_table = de_novo_table.key_by(\n",
    "        'locus', 'alleles').collect_by_key('de_novo_data')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'de_novo_data': array<struct {\n",
      "        id: str, \n",
      "        prior: float64, \n",
      "        proband: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        father: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        mother: struct {\n",
      "            s: str\n",
      "        }, \n",
      "        proband_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        father_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        mother_entry: struct {\n",
      "            AD: array<int32>, \n",
      "            DP: int32, \n",
      "            GQ: int32, \n",
      "            GT: call, \n",
      "            MIN_DP: int32, \n",
      "            PGT: call, \n",
      "            PID: str, \n",
      "            PL: array<int32>, \n",
      "            PS: int32, \n",
      "            RGQ: int32, \n",
      "            SB: array<int32>\n",
      "        }, \n",
      "        is_female: bool, \n",
      "        p_de_novo: float64, \n",
      "        confidence: str\n",
      "    }> \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "de_novo_table.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td>de_novo_data</td></tr>\n",
       "<tr><td>locus&lt;GRCh38&gt;</td><td>array&lt;str&gt;</td><td>array&lt;struct{id: str, prior: float64, proband: struct{s: str}, father: struct{s: str}, mother: struct{s: str}, proband_entry: struct{AD: array&lt;int32&gt;, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array&lt;int32&gt;, PS: int32, RGQ: int32, SB: array&lt;int32&gt;}, father_entry: struct{AD: array&lt;int32&gt;, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array&lt;int32&gt;, PS: int32, RGQ: int32, SB: array&lt;int32&gt;}, mother_entry: struct{AD: array&lt;int32&gt;, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array&lt;int32&gt;, PS: int32, RGQ: int32, SB: array&lt;int32&gt;}, is_female: bool, p_de_novo: float64, confidence: str}&gt;</td></tr>\n",
       "</thead><tbody><tr><td>chr1:16959</td><td>[&quot;G&quot;,&quot;C&quot;]</td><td>[(&quot;EGAN00001315776&quot;,3.36e-05,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17375</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>[(&quot;EGAN00001343225&quot;,1.34e-02,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17407</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[(&quot;EGAN00001343225&quot;,5.75e-03,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17452</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>[(&quot;EGAN00001313169&quot;,3.21e-03,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17487</td><td>[&quot;C&quot;,&quot;A&quot;]</td><td>[(&quot;EGAN00001315786&quot;,3.33e-06,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17512</td><td>[&quot;C&quot;,&quot;G&quot;]</td><td>[(&quot;EGAN00001315333&quot;,4.13e-04,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:17519</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>[(&quot;EGAN00001324367&quot;,2.20e-02,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:63628</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>[(&quot;EGAN00001342824&quot;,3.33e-06,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:133383</td><td>[&quot;A&quot;,&quot;ACT&quot;]</td><td>[(&quot;EGAN00001342875&quot;,3.33e-06,(&quot;EGAN00...</td></tr>\n",
       "<tr><td>chr1:135166</td><td>[&quot;T&quot;,&quot;TGAGGCC&quot;]</td><td>[(&quot;EGAN00001319138&quot;,2.01e-05,(&quot;EGAN00...</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+-----------------+------------------------------------------+\n",
       "| locus         | alleles         | de_novo_data                             |\n",
       "+---------------+-----------------+------------------------------------------+\n",
       "| locus<GRCh38> | array<str>      | array<struct{id: str, prior: float64,... |\n",
       "+---------------+-----------------+------------------------------------------+\n",
       "| chr1:16959    | [\"G\",\"C\"]       | [(\"EGAN00001315776\",3.36e-05,(\"EGAN00... |\n",
       "| chr1:17375    | [\"A\",\"G\"]       | [(\"EGAN00001343225\",1.34e-02,(\"EGAN00... |\n",
       "| chr1:17407    | [\"G\",\"A\"]       | [(\"EGAN00001343225\",5.75e-03,(\"EGAN00... |\n",
       "| chr1:17452    | [\"C\",\"T\"]       | [(\"EGAN00001313169\",3.21e-03,(\"EGAN00... |\n",
       "| chr1:17487    | [\"C\",\"A\"]       | [(\"EGAN00001315786\",3.33e-06,(\"EGAN00... |\n",
       "| chr1:17512    | [\"C\",\"G\"]       | [(\"EGAN00001315333\",4.13e-04,(\"EGAN00... |\n",
       "| chr1:17519    | [\"G\",\"T\"]       | [(\"EGAN00001324367\",2.20e-02,(\"EGAN00... |\n",
       "| chr1:63628    | [\"G\",\"A\"]       | [(\"EGAN00001342824\",3.33e-06,(\"EGAN00... |\n",
       "| chr1:133383   | [\"A\",\"ACT\"]     | [(\"EGAN00001342875\",3.33e-06,(\"EGAN00... |\n",
       "| chr1:135166   | [\"T\",\"TGAGGCC\"] | [(\"EGAN00001319138\",2.01e-05,(\"EGAN00... |\n",
       "+---------------+-----------------+------------------------------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "de_novo_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
